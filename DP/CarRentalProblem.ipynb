{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jack's car rental\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('C:\\Program Files\\python36\\Lib\\site-packages')\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import Env, spaces\n",
    "from gym.envs.toy_text import discrete\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car rental enviroment\n",
    "# distribution model is difficult, try to establish a sample model\n",
    "\n",
    "def categorical_sample(prob_n, np_random):\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "class CarRentalEnv(Env):\n",
    "    # action means take how many cars from place one to place two, -5 means take 5 cars from place two to place one\n",
    "    def __init__(self, max_cars, max_removeable_cars, isd=None):\n",
    "        self.max_cars = max_cars\n",
    "        self.nS = max_cars ** 2\n",
    "        self.nA = 2 * max_removeable_cars + 1\n",
    "        if isd == None:\n",
    "            self.isd = np.ones(self.nS) / self.nS\n",
    "        self.max_removeable_cars = max_removeable_cars\n",
    "        self.shape = (max_cars, max_cars)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "\n",
    "        self.seed()\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        return self.s\n",
    "    \n",
    "    def step(self,a):\n",
    "        x, y = np.unravel_index(self.s, self.shape)\n",
    "        # resize action\n",
    "        if a - self.max_removeable_cars > x:\n",
    "            a = self.max_removeable_cars + x\n",
    "        if self.max_removeable_cars - a > y:\n",
    "            a = self.max_removeable_cars - y\n",
    "        available_x = x - (a - self.max_removeable_cars)\n",
    "        available_y = y + (a - self.max_removeable_cars)\n",
    "        # sample rent cars and return cars\n",
    "        rent_cars_in_x = np.random.poisson(3)\n",
    "        print(f\"{rent_cars_in_x} cars wanted in place one\")\n",
    "        return_cars_in_x = np.random.poisson(3)\n",
    "        print(f\"{return_cars_in_x} cars returned in place one\")\n",
    "        actual_rent_cars_in_x = min(rent_cars_in_x, available_x)\n",
    "        print(f\"{actual_rent_cars_in_x} cars rented actually\")\n",
    "        newx = min(available_x - actual_rent_cars_in_x + return_cars_in_x, self.max_cars)\n",
    "        print(f\"newx is {newx}\")\n",
    "              \n",
    "        rent_cars_in_y = np.random.poisson(4)\n",
    "        print(f\"{rent_cars_in_y} cars wanted in place two\")\n",
    "        return_cars_in_y = np.random.poisson(2)\n",
    "        print(f\"{return_cars_in_y} cars returned in place two\")\n",
    "        actual_rent_cars_in_y = min(rent_cars_in_y, available_y)\n",
    "        print(f\"{actual_rent_cars_in_y} cars rented actually\")\n",
    "        newy = min(available_y - actual_rent_cars_in_y + return_cars_in_y, self.max_cars)\n",
    "        print(f\"newy is {newy}\")\n",
    "              \n",
    "        s = np.ravel_multi_index((newx, newy), self.shape)\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        rewards = 10 * (actual_rent_cars_in_x + actual_rent_cars_in_y) - 2 * abs(a - self.max_removeable_cars)\n",
    "        # output infomation\n",
    "        if a < self.max_removeable_cars:\n",
    "            print(f\"Action: Take {self.max_removeable_cars - a} cars  from place two to place one\")\n",
    "        elif a == self.max_removeable_cars:\n",
    "            print(f\"Action: No cars are moved\")\n",
    "        else:\n",
    "            print(f\"Action: Take {a - self.max_removeable_cars} cars from place two to place one\")\n",
    "        print(f\"Next State is ({newx},{newy})\")\n",
    "        return (s, rewards, False, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic env\n",
    "class CarRentEnvV2(discrete.DiscreteEnv):\n",
    "    \n",
    "    def __init__(self, max_cars=20, max_moveable_cars=5):\n",
    "        self.shape = (max_cars + 1, max_cars + 1)\n",
    "        self.max_cars = max_cars\n",
    "        self.moveable_cars = max_moveable_cars\n",
    "        self.rent_prob_at_1 = self._init_prob(3)\n",
    "        self.rent_prob_at_2 = self._init_prob(4)\n",
    "        self.rent_prob_at_1_bound = self._init_prob_bound(3)\n",
    "        self.rent_prob_at_2_bound = self._init_prob_bound(4)\n",
    "        self.return_prob_at_1 = self._init_prob(3)\n",
    "        self.return_prob_at_2 = self._init_prob(2)\n",
    "        self.return_prob_at_1_bound = self._init_prob_bound(3)\n",
    "        self.return_prob_at_2_bound = self._init_prob_bound(2)\n",
    "        nS = np.prod(self.shape)\n",
    "        nA = 2 * max_moveable_cars + 1\n",
    "        \n",
    "        P = {}\n",
    "        # calculate the prob for each s,a :(pb, next_state, reward, done)\n",
    "        for state in range(nS):\n",
    "            P[state] = {a:[] for a in range(nA)}\n",
    "            x, y = np.unravel_index(state, self.shape)\n",
    "            # calculate the range of actions\n",
    "            max_action = self.moveable_cars + min(self.moveable_cars, x)\n",
    "            min_action = self.moveable_cars - min(self.moveable_cars, y)\n",
    "            for action in range(min_action, max_action + 1):\n",
    "                a = action - self.moveable_cars\n",
    "                available_x = x - a\n",
    "                available_y = y + a\n",
    "                for rent_1 in range(available_x + 1):\n",
    "                    max_return_at_1 = self.max_cars - (available_x - rent_1)\n",
    "                    prob_rent_1 = self.rent_prob_at_1[rent_1] if rent_1 != available_x else self.rent_prob_at_1_bound[available_x]\n",
    "                    for rent_2 in range(available_y + 1):\n",
    "                        max_return_at_2 = self.max_cars - (available_y - rent_2)\n",
    "                        prob_rent_2 = self.rent_prob_at_2[rent_2] if rent_2 != available_y else self.rent_prob_at_2_bound[available_y]\n",
    "                        for return_1 in range(max_return_at_1 + 1):\n",
    "                            prob_return_1 = self.return_prob_at_1[return_1] if return_1 != max_return_at_1 else self.return_prob_at_1_bound[max_return_at_1]\n",
    "                            for return_2 in range(max_return_at_2 + 1):\n",
    "                                prob_return_2 = self.return_prob_at_2[return_2] if return_2 != max_return_at_2 else self.return_prob_at_2_bound[max_return_at_2]\n",
    "                                # cal next state\n",
    "                                newx = available_x - rent_1 + return_1\n",
    "                                newy = available_y - rent_2 + return_2\n",
    "                                assert newx <= self.max_cars, f\"error: newx:{newx}\"\n",
    "                                assert newy <= self.max_cars, f\"error: newy:{newy}\"\n",
    "                                next_state = np.ravel_multi_index((newx, newy), self.shape)\n",
    "                                # cal prob\n",
    "                                prob = prob_rent_1 * prob_rent_2 * prob_return_1 * prob_return_2\n",
    "                                # cal reward\n",
    "                                reward = 10 * rent_1 + 10 * rent_2 - 2 * abs(a)\n",
    "                                P[state][action].append((prob, next_state, reward, False))\n",
    "        isd = np.ones(nS)/ nS\n",
    "        super().__init__(nS, nA, P, isd)\n",
    "    \n",
    "    def _render(self):\n",
    "        pass\n",
    "    \n",
    "    # these array speed up the calculation\n",
    "    def _init_prob(self, lam):\n",
    "        prob_array = np.zeros(self.max_cars * 2 + 1)\n",
    "        for i in range(self.max_cars * 2 + 1):\n",
    "            prob_array[i] = poisson.pmf(i, lam)\n",
    "        return prob_array\n",
    "    \n",
    "    # arr[i] means the probality that greater than or equal \n",
    "    def _init_prob_bound(self, lam):\n",
    "        # prob_bound_array[0] must be 1\n",
    "        prob_bound_array = np.ones(self.max_cars * 2 + 1)\n",
    "        for i in range(1, self.max_cars * 2 + 1):\n",
    "            prob_bound_array[i] = 1 - poisson.cdf(i - 1, lam)\n",
    "        return prob_bound_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_policy(env):\n",
    "    moveable_cars = env.moveable_cars\n",
    "    policy = np.random.randint(env.nA, size=env.shape)\n",
    "    for i in range(env.shape[0]):\n",
    "        for j in range(env.shape[1]):\n",
    "            if policy[i][j] - moveable_cars > i:\n",
    "                policy[i][j] = moveable_cars + i\n",
    "            if moveable_cars - policy[i][j] > j:\n",
    "                policy[i][j] = moveable_cars -j\n",
    "                \n",
    "    return policy - moveable_cars\n",
    "    \n",
    "def policy_eval(env, policy, theta=0.01, gamma = 0.9):\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            x, y = np.unravel_index(state, env.shape)\n",
    "            v = 0\n",
    "            a = policy[x][y] + env.moveable_cars\n",
    "            for re_iter in env.P[state][a]:\n",
    "                prob, next_state, reward, _ = re_iter\n",
    "                next_x, next_y = np.unravel_index(next_state, env.shape)\n",
    "                v += prob * (reward  + gamma * V[next_x][next_y])\n",
    "            delta = max(delta, abs(v - V[x][y]))\n",
    "            V[x][y] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improve(env, V, gamma=0.9):\n",
    "    policy = np.zeros(env.shape)\n",
    "    def look_onestep_ahead(state, V):\n",
    "        A = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for re_iter in env.P[state][action]:\n",
    "                prob, next_state, reward, _ = re_iter\n",
    "                next_x, next_y = np.unravel_index(next_state, env.shape)\n",
    "                A[action] += prob * (reward  + gamma * V[next_x][next_y])\n",
    "            \n",
    "        return A\n",
    "    for i in range(env.max_cars + 1):\n",
    "        for j in range(env.max_cars + 1):\n",
    "            state = np.ravel_multi_index((i, j), env.shape)\n",
    "            A = look_onestep_ahead(state, V)\n",
    "            best_a = np.argmax(A) - env.moveable_cars\n",
    "            policy[i][j] = best_a\n",
    "        \n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env):\n",
    "    my_policy = random_init_policy(env)\n",
    "    while True:\n",
    "        policy_stable = True\n",
    "        V = policy_eval(env, my_policy)\n",
    "        policy = policy_improve(env, V)\n",
    "        for i in range(policy.shape[0]):\n",
    "            for j in range(policy.shape[1]):\n",
    "                if policy[i][j] != my_policy[i][j]:\n",
    "                    policy_stable = False\n",
    "                    break\n",
    "        if policy_stable:\n",
    "            break\n",
    "        my_policy = policy\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env done!\n"
     ]
    }
   ],
   "source": [
    "# policy iteration to solve the car rental problem\n",
    "env = CarRentEnvV2(20, 5)\n",
    "print(\"env done!\")\n",
    "policy, V = policy_iteration(env)\n",
    "\n",
    "np.save('policy', policy)\n",
    "np.save('V', V)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
